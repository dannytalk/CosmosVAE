{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-jwuBfWplDZB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras import layers, Input, Model, ops\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BCnAMU5lDZE"
   },
   "source": [
    "We take the pickle file (`.pkl`) containing the images (in this workbook, from the A1 tile) and the `csv` file containing the redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "C5b2oeynlDZE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"pickles/A1_compiled_cutouts_3arcsec_30mas.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2xlVRoiUlDZF",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickles\\\\A1_compiled_cutouts_3arcsec_30mas.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# rows: ID, image info\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(DATA_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickles\\\\A1_compiled_cutouts_3arcsec_30mas.pkl'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# rows: ID, image info\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs\n",
    "ids = data[0]\n",
    "\n",
    "# images\n",
    "images = np.stack(data[1]).astype(\"float32\")\n",
    "images = np.log10(images - np.min(images)+0.007) # log transform to reduce differences in scale\n",
    "images = images / np.max(images)  # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oczGWx9mlDZG"
   },
   "source": [
    "Match and merge the datasets based on the object's ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmBIzomnlDZH"
   },
   "outputs": [],
   "source": [
    "attrs = pd.read_csv(\"cosmos_cut.csv\", sep=\",\")\n",
    "\n",
    "df_images = pd.DataFrame({'id': ids}) # from ID numbers\n",
    "df_merged = pd.merge(df_images, attrs, on=\"id\", how=\"inner\") # only when IDs match\n",
    "\n",
    "images = images[df_merged.index]\n",
    "redshifts = df_merged[\"z\"].to_numpy(dtype=\"float32\")\n",
    "redshifts = redshifts / np.max(redshifts) # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data to extend AGN and LRD parts of the database so that the dataset is more balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_merged[\"classification\"].to_numpy()\n",
    "obj_ids = df_merged[\"id\"].to_numpy()\n",
    "\n",
    "aug_images = []\n",
    "aug_ids = []\n",
    "aug_redshifts = []\n",
    "\n",
    "for img, obj_id, z, cls in zip(images, obj_ids, redshifts, classes):\n",
    "    if cls == \"Galaxy\":\n",
    "        # keep as-is, no augmentation\n",
    "        aug_images.append(img)\n",
    "        aug_ids.append(obj_id)\n",
    "        aug_redshifts.append(z)\n",
    "\n",
    "    elif cls == \"AGN\":\n",
    "        # rotations -> 4 total (0°, 90°, 180°, 270°)\n",
    "        variants = [\n",
    "            img,\n",
    "            np.rot90(img, 1, axes=(0, 1)),\n",
    "            np.rot90(img, 2, axes=(0, 1)),\n",
    "            np.rot90(img, 3, axes=(0, 1)),\n",
    "        ]\n",
    "        for v in variants:\n",
    "            aug_images.append(v)\n",
    "            aug_ids.append(obj_id)\n",
    "            aug_redshifts.append(z)\n",
    "\n",
    "    elif cls == \"LRD\":\n",
    "        # rotations + flips (more aggressive augmentation)\n",
    "        base_rots = [\n",
    "            img,\n",
    "            np.rot90(img, 1, axes=(0, 1)),\n",
    "            np.rot90(img, 2, axes=(0, 1)),\n",
    "            np.rot90(img, 3, axes=(0, 1)),\n",
    "        ]\n",
    "        variants = base_rots + [\n",
    "            np.fliplr(img),\n",
    "            np.flipud(img),\n",
    "            np.fliplr(base_rots[1]),  # flipped 90°\n",
    "            np.flipud(base_rots[1]),  # flipped 90°\n",
    "        ]\n",
    "        for v in variants:\n",
    "            aug_images.append(v)\n",
    "            aug_ids.append(obj_id)\n",
    "            aug_redshifts.append(z)\n",
    "\n",
    "    else:\n",
    "        # any other class: just keep original\n",
    "        aug_images.append(img)\n",
    "        aug_ids.append(obj_id)\n",
    "        aug_redshifts.append(z)\n",
    "\n",
    "# -------------------------\n",
    "# final augmented datasets\n",
    "# -------------------------\n",
    "aug_images = np.stack(aug_images).astype(\"float32\")\n",
    "aug_ids = np.array(aug_ids)\n",
    "aug_redshifts = np.array(aug_redshifts, dtype=\"float32\")\n",
    "\n",
    "print(\"Original size:\", len(images))\n",
    "print(\"Augmented size:\", len(aug_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_g1T8-JlDZJ"
   },
   "source": [
    "## Constructing the autoencoder\n",
    "\n",
    "Since each image is of dimension $(100, 100, 4)$, at the raw scale, they are respresented by $100 \\cdot 100 \\cdot 4 = 40,000$ values. We choose the number of dimensions to reduce the size of the vector that encodes each input in the latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFHvWTjilDZJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJXVQCK6lDZJ"
   },
   "source": [
    "The encoder part of the VAE architecture maps a data point to a variational mean and (log) variance. The mean is a point in the latent space.\n",
    "The \"reparameterization trick\" draws samples from the variational distribution that are parameterized by the variational mean ($\\mu$) and variance ($\\sigma^2$), with error $\\epsilon$, so that the parameters of the encoder network can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Inputs\n",
    "img_in = layers.Input(shape=(100, 100, 4), name=\"image\")\n",
    "z_in   = layers.Input(shape=(1,),          name=\"redshift\")\n",
    "\n",
    "# ---- Image branch\n",
    "x = layers.Conv2D(64, 5, padding=\"same\", activation=\"relu\")(img_in)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# ---- Concatenate image features + redshift\n",
    "hg = layers.Dense(512, activation=\"relu\")(z_in)\n",
    "h = layers.Concatenate()([x, hg])\n",
    "#h = layers.Dense(512, activation=\"relu\")(h)\n",
    "\n",
    "# ---- Latent parameters\n",
    "z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(h)\n",
    "z_logv = layers.Dense(LATENT_DIM, name=\"z_logvar\")(h)\n",
    "\n",
    "# ---- Sampling layer that ALSO adds KL via add_loss\n",
    "class Sampling(layers.Layer):\n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Sampling layer for a VAE, applying the reparameterization trick.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        beta : float\n",
    "            Weight applied to the KL divergence term (β-VAE).  \n",
    "            - beta = 1 → standard VAE  \n",
    "            - beta > 1 → more regularized latent space  \n",
    "            - beta < 1 → allow more expressive latent\n",
    "            \n",
    "        kwargs : passed to layers.Layer\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.rng = keras.random.SeedGenerator(1337) # for reproducibility\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # unpack encoder outputs\n",
    "        mean, logvar = inputs\n",
    "\n",
    "        # ---- (1) Sample ε ~ N(0, I) with same shape as μ\n",
    "        eps = keras.random.normal(shape=ops.shape(mean),\n",
    "                                  dtype=mean.dtype,\n",
    "                                  seed=self.rng)\n",
    "\n",
    "        # ---- (2) Reparameterization trick\n",
    "        # Convert log-variance → standard deviation: σ = exp(0.5 * logσ²)\n",
    "        # Then sample: z = μ + σ * ε\n",
    "        z = mean + ops.exp(0.5 * logvar) * eps\n",
    "\n",
    "        # KL per sample\n",
    "        kl_per_sample = -0.5 * ops.sum(\n",
    "            1 + logvar - ops.square(mean) - ops.exp(logvar), axis=-1\n",
    "        )\n",
    "        # Apply weighting factor (β-VAE trick)\n",
    "        self.add_loss(self.beta * ops.mean(kl_per_sample))\n",
    "        return z\n",
    "\n",
    "# ---- Encoder model\n",
    "encoder = Model(inputs=[img_in, z_in],\n",
    "                outputs=z_latent,\n",
    "                name=\"encoder\")\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---- Decoder (hybrid upsampling + convolution)\n",
    "\n",
    "# inputs\n",
    "d_inputs = Input(shape=(LATENT_SPACE_SIZE,), name='latent_input')\n",
    "\n",
    "d = layers.Dense(12 * 12 * 64, activation=\"relu\")(z_latent) \n",
    "d = layers.Reshape((12, 12, 64))(d)  # start from 12x12\n",
    "\n",
    "# 12 -> 24\n",
    "d = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(d)\n",
    "d = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(d)\n",
    "\n",
    "# 24 -> 48\n",
    "d = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(d)\n",
    "d = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(d)\n",
    "\n",
    "# 48 -> 96\n",
    "d = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(d)\n",
    "d = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(d)\n",
    "\n",
    "# Final adjustment: 96 -> 100\n",
    "d = layers.Resizing(100, 100, interpolation=\"bilinear\")(d)\n",
    "\n",
    "# Output reconstruction (linear activation since it's regression)\n",
    "x_recon = layers.Conv2D(4, 3, padding=\"same\", activation=\"linear\", name=\"image_recon\")(d)\n",
    "\n",
    "# ---- Optional small head to reconstruct the scalar redshift as an auxiliary target\n",
    "z_hat = layers.Dense(32, activation=\"relu\")(z_latent)\n",
    "z_hat = layers.Dense(1,  activation=\"linear\", name=\"z_recon\")(z_hat)\n",
    "\n",
    "decoder = Model(z_latent, [x_recon, z_hat])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = keras.Model(inputs=[img_in, z_in], outputs=[x_recon, z_hat], name=\"vae\")\n",
    "\n",
    "# Only reconstruction losses go in compile. The KL is already attached via add_loss.\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss={\"image_recon\": \"mse\", \"z_recon\": \"mse\"},\n",
    "    loss_weights={\"image_recon\": 1000.0, \"z_recon\": 1.0},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model has two main components to its loss function: the reconstruction loss (how well the decoder rebuilds the input features from the latent embeddings) and the KL-divergence loss (how much the encoder's latent distribution deviates from the target prior, usually a standard Gaussian which forces a smooth and continuous latent space). In our architecture, we included the parameter $\\beta$ to finetune this tradeoff. \n",
    "\n",
    "In addition, the image reconstruction is weighted considerably higher in the loss function than the redshift reconstruction, as it is easier to minimize loss on one scalar than 100x100 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary() # overall architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation sets\n",
    "X_train, X_val, z_train, z_val = train_test_split(aug_images, aug_redshifts, test_size=0.2, random_state=42)\n",
    "\n",
    "z_train = z_train.reshape(-1, 1)\n",
    "z_val = z_val.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W8Z7sZflDZV"
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    [X_train, z_train],               # inputs: images + redshift\n",
    "    [X_train, z_train],               # targets: reconstruct both\n",
    "    epochs=1,                         # INCREASE EPOCH COUNT FOR BETTER TRAINING\n",
    "    batch_size=64,\n",
    "    validation_data=([X_val, z_val], [X_val, z_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def _asinh_stretch(x, scale=None):\n",
    "    \"\"\"\n",
    "    Asinh stretch commonly used for astro images.\n",
    "    If scale is None, use a robust estimate based on the 90th percentile of |x|.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if scale is None:\n",
    "        p = np.nanpercentile(np.abs(x), 90)\n",
    "        scale = p if p > 0 else np.nanmax(np.abs(x)) + 1e-8\n",
    "    return np.arcsinh(x / (scale + 1e-12))\n",
    "\n",
    "def _auto_vmin_vmax(x, pct=(1, 99)):\n",
    "    \"\"\"Robust display limits from percentiles.\"\"\"\n",
    "    lo, hi = np.nanpercentile(x, pct[0]), np.nanpercentile(x, pct[1])\n",
    "    if hi <= lo:  # fallback\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        lo, hi = m - 2*s, m + 2*s\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def show_input_vs_output(\n",
    "    model,\n",
    "    X,\n",
    "    z,\n",
    "    idx=0,\n",
    "    channel_names=None,\n",
    "    stretch=\"asinh\",      # 'asinh' or 'linear'\n",
    "    percentiles=(1, 99),  # for linear scaling\n",
    "    figsize=(12, 6),\n",
    "    cmap=\"gray\",\n",
    "    show_residuals=True,\n",
    "    print_redshift=True):\n",
    "    \"\"\"\n",
    "    Visualize original vs reconstructed image for a single (image, redshift) pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.Model\n",
    "        Your trained VAE model with outputs [image_recon, z_recon] or [image_recon].\n",
    "    X : np.ndarray\n",
    "        Image array of shape (N, H, W, C). (C should be 4 for your use case.)\n",
    "    z : np.ndarray\n",
    "        Redshift array of shape (N, 1) or (N,).\n",
    "    idx : int\n",
    "        Index of the sample to visualize.\n",
    "    channel_names : list[str] or None\n",
    "        Optional names for channels (e.g., [\"F115W\",\"F150W\",\"F277W\",\"F444W\"]).\n",
    "    stretch : {'asinh', 'linear'}\n",
    "        Display stretch for visualization.\n",
    "    percentiles : tuple(int,int)\n",
    "        For 'linear' stretch, the (low, high) percentiles to set vmin/vmax.\n",
    "    figsize : tuple\n",
    "        Figure size.\n",
    "    cmap : str\n",
    "        Matplotlib colormap.\n",
    "    show_residuals : bool\n",
    "        If True, shows a third row with (recon - input).\n",
    "    print_redshift : bool\n",
    "        If True, prints the input redshift and predicted redshift (if available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        'x_true', 'x_recon', 'z_true', 'z_pred', 'residual'\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the specific sample\n",
    "    x_true = X[idx]\n",
    "    z_true = z[idx]\n",
    "    if z_true.ndim == 0:        # scalar -> (1,)\n",
    "        z_true = np.array([z_true], dtype=np.float32)\n",
    "    if z_true.ndim == 1:        # (1,) -> (1,1) when batching\n",
    "        z_true_b = z_true.reshape(1, 1)\n",
    "    else:\n",
    "        z_true_b = z_true.reshape(1, *z_true.shape)\n",
    "\n",
    "    x_true_b = x_true[np.newaxis, ...]  # (1,H,W,C)\n",
    "\n",
    "    # Forward pass\n",
    "    pred = model.predict([x_true_b, z_true_b], verbose=0)\n",
    "    if isinstance(pred, (list, tuple)):\n",
    "        x_recon_b = pred[0]\n",
    "        z_pred_b = pred[1] if len(pred) > 1 else None\n",
    "    else:\n",
    "        x_recon_b = pred\n",
    "        z_pred_b = None\n",
    "\n",
    "    x_recon = np.squeeze(x_recon_b, axis=0)  # (H,W,C)\n",
    "    residual = x_recon - x_true\n",
    "\n",
    "    # Print redshifts if desired\n",
    "    if print_redshift:\n",
    "        try:\n",
    "            zt_scalar = float(z_true.ravel()[0])\n",
    "        except Exception:\n",
    "            zt_scalar = np.nan\n",
    "        if z_pred_b is not None:\n",
    "            zp_scalar = float(np.squeeze(z_pred_b))\n",
    "            print(f\"z_true = {zt_scalar:.5f} | z_pred = {zp_scalar:.5f}\")\n",
    "        else:\n",
    "            print(f\"z_true = {zt_scalar:.5f} | (model has no z_pred head)\")\n",
    "\n",
    "    H, W, C = x_true.shape\n",
    "    cols = C\n",
    "    rows = 3 if show_residuals else 2\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, constrained_layout=True)\n",
    "    if rows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    if cols == 1:\n",
    "        axes = np.expand_dims(axes, 1)\n",
    "\n",
    "    def format_title(base, c):\n",
    "        if channel_names and c < len(channel_names):\n",
    "            return f\"{base} – {channel_names[c]}\"\n",
    "        return f\"{base} – ch{c}\"\n",
    "\n",
    "    for c in range(C):\n",
    "        # Choose scaling for each panel\n",
    "        if stretch == \"asinh\":\n",
    "            a_true = _asinh_stretch(x_true[..., c])\n",
    "            a_reco = _asinh_stretch(x_recon[..., c])\n",
    "            vmin_t, vmax_t = _auto_vmin_vmax(a_true)\n",
    "            vmin_r, vmax_r = _auto_vmin_vmax(a_reco)\n",
    "            # Keep separate vmin/vmax for truth vs recon to show structure clearly\n",
    "            axes[0, c].imshow(a_true, cmap=cmap, vmin=vmin_t, vmax=vmax_t)\n",
    "            axes[1, c].imshow(a_reco, cmap=cmap, vmin=vmin_r, vmax=vmax_r)\n",
    "        else:  # linear\n",
    "            vmin, vmax = _auto_vmin_vmax(x_true[..., c], percentiles)\n",
    "            axes[0, c].imshow(x_true[..., c], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            axes[1, c].imshow(x_recon[..., c], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        axes[0, c].set_title(format_title(\"Input\", c))\n",
    "        axes[1, c].set_title(format_title(\"Reconstruction\", c))\n",
    "        axes[0, c].axis(\"off\")\n",
    "        axes[1, c].axis(\"off\")\n",
    "\n",
    "        if show_residuals:\n",
    "            # Residuals in linear scale centered at 0 with symmetric range\n",
    "            res = residual[..., c]\n",
    "            m = np.nanmax(np.abs(res)) + 1e-12\n",
    "            axes[2, c].imshow(res, cmap=cmap, vmin=-m, vmax=m)\n",
    "            axes[2, c].set_title(format_title(\"Residual (recon - input)\", c))\n",
    "            axes[2, c].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"x_true\": x_true,\n",
    "        \"x_recon\": x_recon,\n",
    "        \"z_true\": z_true,\n",
    "        \"z_pred\": None if z_pred_b is None else float(np.squeeze(z_pred_b)),\n",
    "        \"residual\": residual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example channel labels for 4-band data:\n",
    "chan_names = [\"F115W\", \"F150W\", \"F277W\", \"F444W\"]\n",
    "\n",
    "# Show sample (change ID as desired)\n",
    "_ = show_input_vs_output(\n",
    "    autoencoder,\n",
    "    X_train,\n",
    "    z_train,\n",
    "    idx=98,\n",
    "    channel_names=chan_names,\n",
    "    stretch=\"linear\",      \n",
    "    show_residuals=True,\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t56oLcBBlDZW"
   },
   "source": [
    "# Visualize the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlJaJM2wlDZW"
   },
   "source": [
    "Since the latent space is set as 100 per the model architecture, we only select the most distinguishing dimensions to construct pairwise plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatentSpace_pairplot(encoder_model, images, labels, n_dims=10):\n",
    "    # Encode all images to latent space\n",
    "    z = encoder_model.predict(images, batch_size=64)\n",
    "    \n",
    "    # If latent space > n_dims, only take the first few for clarity\n",
    "    z = z[:, :n_dims]\n",
    "    \n",
    "    # Build a dataframe\n",
    "    df = pd.DataFrame(z, columns=[f\"z{i+1}\" for i in range(n_dims)])\n",
    "    df[\"classification\"] = labels\n",
    "\n",
    "    # color and transparency maps\n",
    "    label_to_color = {\"LRD\": \"red\", \"Galaxy\": \"dodgerblue\", \"AGN\": \"mediumseagreen\"}\n",
    "    label_to_alpha = {\"LRD\": 1.0, \"Galaxy\": 0.15, \"AGN\": 0.45}\n",
    "\n",
    "    # Plot\n",
    "    sns.pairplot(df, hue=\"classification\", palette=label_to_color, plot_kws={'alpha':0.4, 's':20})\n",
    "    plt.suptitle(\"Latent Space Pairplot by Classification\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract classification labels\n",
    "labels = df_merged[\"classification\"].to_numpy()\n",
    "\n",
    "# Create visualization\n",
    "LatentSpace_pairplot(encoder, [images, redshifts], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "We use Uniform Manifold Approximation and Projection (UMAP) to reduce the dimensionality of the latent space to a 3D projection while preserving both the global structure (overall shape of the data) and local structure (how nearby points relate to each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have umap\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a known UMAP shadow module in `cuml` which introduces instability and crashes kernels when run on GPU. Run the following block as applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect which UMAP package you're actually using\n",
    "import umap\n",
    "print(\"UMAP path:\", umap.__file__)\n",
    "\n",
    "# If the printed path contains \"cuml\" or \"rapids\", then:\n",
    "# !pip uninstall cuml -y\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,      # local neighborhood size\n",
    "    min_dist=0.1,        # how tightly UMAP packs points\n",
    "    n_components=3,      # output dimensions (2D or 3D)\n",
    "    metric='euclidean')   # distance metric\n",
    "\n",
    "latent_space = encoder.predict([images, redshifts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reduced dimension latent space\n",
    "latent_umap = reducer.fit_transform(latent_space)\n",
    "latent_dims = ['UMAP1', 'UMAP2', 'UMAP3']\n",
    "\n",
    "df_latent = pd.DataFrame(latent_umap, columns=latent_dims)\n",
    "df_latent['classification'] = df_merged['classification']\n",
    "\n",
    "# color + alpha mapping to highlight LRDs\n",
    "label_to_color = {\"LRD\": \"red\", \"Galaxy\": \"dodgerblue\", \"AGN\": \"mediumseagreen\"}\n",
    "label_to_alpha = {\"LRD\": 1.0, \"Galaxy\": 0.15, \"AGN\": 0.45}\n",
    "\n",
    "# pairwise plots \n",
    "plot_pairs = [('UMAP1', 'UMAP2'), ('UMAP2', 'UMAP3'), ('UMAP1', 'UMAP3')]\n",
    "\n",
    "for x_dim, y_dim in plot_pairs:\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    for cls in df_latent['classification'].unique():\n",
    "        subset = df_latent[df_latent['classification'] == cls]\n",
    "        plt.scatter(subset[x_dim], subset[y_dim],\n",
    "                    color=label_to_color[cls],\n",
    "                    alpha=label_to_alpha[cls],\n",
    "                    label=cls,\n",
    "                    s=20)\n",
    "    \n",
    "    plt.xlabel(x_dim)\n",
    "    plt.ylabel(y_dim)\n",
    "    plt.legend()\n",
    "    plt.title(f\"{x_dim} vs {y_dim}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot to see latent dimensions at once\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# create figure and 3D axes\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot for each classification to maintain different alpha levels\n",
    "for cls in df_latent['classification'].unique():\n",
    "    subset = df_latent[df_latent['classification'] == cls]\n",
    "    ax.scatter(subset['UMAP1'], subset['UMAP2'], subset['UMAP3'],\n",
    "                   c=label_to_color[cls],\n",
    "                   alpha=label_to_alpha[cls],\n",
    "                   label=cls,\n",
    "                   s=20)\n",
    "    \n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "ax.set_zlabel('UMAP3')\n",
    "ax.legend(title = 'Classification')\n",
    "ax.set_title('3D UMAP of Latent Space')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare latent space distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Make data frames for separate classes\n",
    "latent_LRD = df_latent[df_latent['classification'] == 'LRD'][['UMAP1','UMAP2','UMAP3']].values\n",
    "latent_Galaxy = df_latent[df_latent['classification'] == 'Galaxy'][['UMAP1','UMAP2','UMAP3']].values\n",
    "latent_AGN = df_latent[df_latent['classification'] == 'AGN'][['UMAP1','UMAP2','UMAP3']].values\n",
    "\n",
    "# Euclidean distances from each LRD to every point in the other groups\n",
    "dist_LRD_Galaxy = cdist(latent_LRD, latent_Galaxy)\n",
    "dist_LRD_AGN = cdist(latent_LRD, latent_AGN)\n",
    "\n",
    "# For each LRD, keep the nearest distance to each group\n",
    "df_LRD = df_latent[df_latent['classification'] == 'LRD'].copy()\n",
    "df_LRD['nearest_galaxy'] = dist_LRD_Galaxy.min(axis=1)\n",
    "df_LRD['nearest_agn'] = dist_LRD_AGN.min(axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "summary_panel = df_LRD[['nearest_galaxy', 'nearest_agn']].describe(percentiles=[.25, .5, .75]).T\n",
    "summary_panel.rename(columns={\n",
    "    '25%': 'Q1',\n",
    "    '50%': 'Median',\n",
    "    '75%': 'Q3'}, inplace=True)\n",
    "summary_panel = summary_panel[['min','Q1','Median','Q3','max','mean','std']]\n",
    "\n",
    "print(summary_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Closer to galaxies! Then again more galaxies than AGNs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

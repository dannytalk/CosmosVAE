{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jwuBfWplDZB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras import layers, Input, Model, ops\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BCnAMU5lDZE"
   },
   "source": [
    "We take the pickle file (`.pkl`) containing the images (in this workbook, from the A1 tile) and the `csv` file containing the redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5b2oeynlDZE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if you make pickle files from the data generator\n",
    "# DATA_PATH = \"pickles/A1_compiled_cutouts_3arcsec_30mas.pkl\"\n",
    "\n",
    "# if you download directly from the drive\n",
    "DATA_PATH = \"A1_compiled_cutouts_3arcsec_30mas.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xlVRoiUlDZF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# rows: ID, image info\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs\n",
    "ids = data[0]\n",
    "\n",
    "# images\n",
    "images = np.stack(data[1]).astype(\"float32\")\n",
    "images = np.log10(images - np.min(images)+0.007) # log transform to reduce differences in scale\n",
    "images = images / np.max(images)  # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oczGWx9mlDZG"
   },
   "source": [
    "Match and merge the datasets based on the object's ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmBIzomnlDZH"
   },
   "outputs": [],
   "source": [
    "attrs = pd.read_csv(\"cosmos_cut.csv\", sep=\",\")\n",
    "\n",
    "df_images = pd.DataFrame({'id': ids}) # from ID numbers\n",
    "df_merged = pd.merge(df_images, attrs, on=\"id\", how=\"inner\") # only when IDs match\n",
    "\n",
    "images = images[df_merged.index]\n",
    "redshifts = df_merged[\"z\"].to_numpy(dtype=\"float32\")\n",
    "redshifts = redshifts / np.max(redshifts) # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data to extend AGN and LRD parts of the database so that the dataset is more balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_merged[\"classification\"].to_numpy()\n",
    "obj_ids = df_merged[\"id\"].to_numpy()\n",
    "\n",
    "aug_images = []\n",
    "aug_ids = []\n",
    "aug_redshifts = []\n",
    "\n",
    "for img, obj_id, z, cls in zip(images, obj_ids, redshifts, classes):\n",
    "    if cls == \"Galaxy\":\n",
    "        # keep as-is, no augmentation\n",
    "        aug_images.append(img)\n",
    "        aug_ids.append(obj_id)\n",
    "        aug_redshifts.append(z)\n",
    "\n",
    "    elif cls == \"AGN\":\n",
    "        # rotations -> 4 total (0°, 90°, 180°, 270°)\n",
    "        variants = [\n",
    "            img,\n",
    "            np.rot90(img, 1, axes=(0, 1)),\n",
    "            np.rot90(img, 2, axes=(0, 1)),\n",
    "            np.rot90(img, 3, axes=(0, 1)),\n",
    "        ]\n",
    "        for v in variants:\n",
    "            aug_images.append(v)\n",
    "            aug_ids.append(obj_id)\n",
    "            aug_redshifts.append(z)\n",
    "\n",
    "    elif cls == \"LRD\":\n",
    "        # rotations + flips (more aggressive augmentation)\n",
    "        base_rots = [\n",
    "            img,\n",
    "            np.rot90(img, 1, axes=(0, 1)),\n",
    "            np.rot90(img, 2, axes=(0, 1)),\n",
    "            np.rot90(img, 3, axes=(0, 1)),\n",
    "        ]\n",
    "        variants = base_rots + [\n",
    "            np.fliplr(img),\n",
    "            np.flipud(img),\n",
    "            np.fliplr(base_rots[1]),  # flipped 90°\n",
    "            np.flipud(base_rots[1]),  # flipped 90°\n",
    "        ]\n",
    "        for v in variants:\n",
    "            aug_images.append(v)\n",
    "            aug_ids.append(obj_id)\n",
    "            aug_redshifts.append(z)\n",
    "\n",
    "    else:\n",
    "        # any other class: just keep original\n",
    "        aug_images.append(img)\n",
    "        aug_ids.append(obj_id)\n",
    "        aug_redshifts.append(z)\n",
    "\n",
    "# -------------------------\n",
    "# final augmented datasets\n",
    "# -------------------------\n",
    "aug_images = np.stack(aug_images).astype(\"float32\")\n",
    "aug_ids = np.array(aug_ids)\n",
    "aug_redshifts = np.array(aug_redshifts, dtype=\"float32\")\n",
    "\n",
    "print(\"Original size:\", len(images))\n",
    "print(\"Augmented size:\", len(aug_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand metadata to include augmented samples\n",
    "# takes all instances of an ID\n",
    "df_aug = pd.DataFrame({\"id\": aug_ids})\n",
    "df_aug = df_aug.merge(df_merged, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_g1T8-JlDZJ"
   },
   "source": [
    "# Load the pre-trained autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom sampling layer\n",
    "@register_keras_serializable(package=\"Custom\") # decorator\n",
    "class Sampling(layers.Layer):\n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.rng = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        eps = keras.random.normal(\n",
    "            shape=ops.shape(mean),\n",
    "            dtype=mean.dtype,\n",
    "            seed=self.rng\n",
    "        )\n",
    "        z = mean + ops.exp(0.5 * logvar) * eps\n",
    "\n",
    "        kl = -0.5 * ops.sum(\n",
    "            1 + logvar - ops.square(mean) - ops.exp(logvar),\n",
    "            axis=-1\n",
    "        )\n",
    "        self.add_loss(self.beta * ops.mean(kl))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = keras.models.load_model(\n",
    "    \"Best_autoencoder.keras\",\n",
    "    custom_objects={\"Sampling\": Sampling},\n",
    "    safe_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation sets\n",
    "X_train, X_val, z_train, z_val = train_test_split(aug_images, aug_redshifts, test_size=0.2, random_state=42)\n",
    "\n",
    "z_train = z_train.reshape(-1, 1)\n",
    "z_val = z_val.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def _asinh_stretch(x, scale=None):\n",
    "    \"\"\"\n",
    "    Asinh stretch commonly used for astro images.\n",
    "    If scale is None, use a robust estimate based on the 90th percentile of |x|.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if scale is None:\n",
    "        p = np.nanpercentile(np.abs(x), 90)\n",
    "        scale = p if p > 0 else np.nanmax(np.abs(x)) + 1e-8\n",
    "    return np.arcsinh(x / (scale + 1e-12))\n",
    "\n",
    "def _auto_vmin_vmax(x, pct=(1, 99)):\n",
    "    \"\"\"\n",
    "    Robust display limits from percentiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Image (2D) or any array to compute limits from.\n",
    "    pct : (low, high)\n",
    "        Percentiles to use, e.g. (1, 99).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vmin, vmax : float\n",
    "        Suggested display limits.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    lo, hi = np.nanpercentile(x, pct)\n",
    "    if hi <= lo:  # fallback if data are weird/constant\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        lo, hi = m - 2 * s, m + 2 * s\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def show_input_vs_output(\n",
    "    model,\n",
    "    X,\n",
    "    z,\n",
    "    labels,\n",
    "    idx=0,\n",
    "    channel_names=None,\n",
    "    stretch=\"asinh\",      # 'asinh' or 'linear'\n",
    "    percentiles=(1, 99),  # for linear scaling\n",
    "    figsize=(12, 6),\n",
    "    cmap=\"gray\",\n",
    "    show_residuals=True,\n",
    "    print_redshift=True):\n",
    "    \"\"\"\n",
    "    Visualize original vs reconstructed image for a single (image, redshift) pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the specific sample\n",
    "    x_true = X[idx]\n",
    "    z_true = z[idx]\n",
    "    if z_true.ndim == 0:        # scalar -> (1,)\n",
    "        z_true = np.array([z_true], dtype=np.float32)\n",
    "    if z_true.ndim == 1:        # (1,) -> (1,1) when batching\n",
    "        z_true_b = z_true.reshape(1, 1)\n",
    "    else:\n",
    "        z_true_b = z_true.reshape(1, *z_true.shape)\n",
    "\n",
    "    x_true_b = x_true[np.newaxis, ...]  # (1,H,W,C)\n",
    "\n",
    "    # Forward pass\n",
    "    pred = model.predict([x_true_b, z_true_b], verbose=0)\n",
    "    if isinstance(pred, (list, tuple)):\n",
    "        x_recon_b = pred[0]\n",
    "        z_pred_b = pred[1] if len(pred) > 1 else None\n",
    "    else:\n",
    "        x_recon_b = pred\n",
    "        z_pred_b = None\n",
    "\n",
    "    x_recon = np.squeeze(x_recon_b, axis=0)  # (H,W,C)\n",
    "    residual = x_recon - x_true\n",
    "\n",
    "    # --- Redshift scalars (for printing and plotting) ----------------------\n",
    "    try:\n",
    "        zt_scalar = float(z_true.ravel()[0])\n",
    "    except Exception:\n",
    "        zt_scalar = np.nan\n",
    "\n",
    "    if z_pred_b is not None:\n",
    "        zp_scalar = float(np.squeeze(z_pred_b))\n",
    "    else:\n",
    "        zp_scalar = None\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    # Print redshifts if desired\n",
    "    if print_redshift:\n",
    "        if zp_scalar is not None:\n",
    "            print(f\"z_true = {zt_scalar:.5f} | z_pred = {zp_scalar:.5f}\")\n",
    "        else:\n",
    "            print(f\"z_true = {zt_scalar:.5f} | (model has no z_pred head)\")\n",
    "\n",
    "    H, W, C = x_true.shape\n",
    "    cols = C\n",
    "    rows = 3 if show_residuals else 2\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, constrained_layout=True)\n",
    "    if rows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    if cols == 1:\n",
    "        axes = np.expand_dims(axes, 1)\n",
    "\n",
    "    # <<< NEW: put z_true / z_pred at top of the figure\n",
    "    if zp_scalar is not None:\n",
    "        suptitle_str = f\" Object type is {labels[idx]} | z_true = {zt_scalar:.5f} | z_pred = {zp_scalar:.5f}\"\n",
    "    else:\n",
    "        suptitle_str = f\"z_true = {zt_scalar:.5f} | (no z_pred)\"\n",
    "    fig.suptitle(suptitle_str, fontsize=14)\n",
    "    # >>>\n",
    "\n",
    "    def format_title(base, c):\n",
    "        if channel_names and c < len(channel_names):\n",
    "            return f\"{base} – {channel_names[c]}\"\n",
    "        return f\"{base} – ch{c}\"\n",
    "\n",
    "    for c in range(C):\n",
    "        # Choose scaling for each panel\n",
    "        if stretch == \"asinh\":\n",
    "            a_true = _asinh_stretch(x_true[..., c])\n",
    "            a_reco = _asinh_stretch(x_recon[..., c])\n",
    "            vmin_t, vmax_t = _auto_vmin_vmax(a_true)\n",
    "            vmin_r, vmax_r = _auto_vmin_vmax(a_reco)\n",
    "            # Keep separate vmin/vmax for truth vs recon to show structure clearly\n",
    "            axes[0, c].imshow(a_true, cmap=cmap, vmin=vmin_t, vmax=vmax_t)\n",
    "            axes[1, c].imshow(a_reco, cmap=cmap, vmin=vmin_r, vmax=vmax_r)\n",
    "        else:  # linear\n",
    "            vmin, vmax = _auto_vmin_vmax(x_true[..., c], percentiles)\n",
    "            axes[0, c].imshow(x_true[..., c], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            axes[1, c].imshow(x_recon[..., c], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        axes[0, c].set_title(format_title(\"Input\", c))\n",
    "        axes[1, c].set_title(format_title(\"Reconstruction\", c))\n",
    "        axes[0, c].axis(\"off\")\n",
    "        axes[1, c].axis(\"off\")\n",
    "\n",
    "        if show_residuals:\n",
    "            # Residuals in linear scale centered at 0 with symmetric range\n",
    "            res = residual[..., c]\n",
    "            m = np.nanmax(np.abs(res)) + 1e-12\n",
    "            axes[2, c].imshow(res, cmap=cmap, vmin=-m, vmax=m)\n",
    "            axes[2, c].set_title(format_title(\"Residual (recon - input)\", c))\n",
    "            axes[2, c].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"x_true\": x_true,\n",
    "        \"x_recon\": x_recon,\n",
    "        \"z_true\": z_true,\n",
    "        \"z_pred\": zp_scalar,\n",
    "        \"residual\": residual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example channel labels for 4-band data:\n",
    "chan_names = [\"F115W\", \"F150W\", \"F277W\", \"F444W\"]\n",
    "\n",
    "# Extract classification labels\n",
    "labels = df_aug[\"classification\"].to_numpy()\n",
    "\n",
    "# Show sample (change ID as desired)\n",
    "_ = show_input_vs_output(\n",
    "    autoencoder,\n",
    "    aug_images,\n",
    "    aug_redshifts,\n",
    "    labels,\n",
    "    31,\n",
    "    channel_names=chan_names,\n",
    "    stretch=\"linear\",      \n",
    "    show_residuals=True,\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t56oLcBBlDZW"
   },
   "source": [
    "# Visualize the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlJaJM2wlDZW"
   },
   "source": [
    "Since the latent space is set as 100 per the model architecture, we only select the most distinguishing dimensions to construct pairwise plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract encoder from autoencoder\n",
    "# inputs to the encoder are just the inputs to the autoencoder\n",
    "img_in_loaded, z_in_loaded = autoencoder.inputs\n",
    "\n",
    "# latent output is the output of the \"z\" layer (Sampling)\n",
    "z_latent_loaded = autoencoder.get_layer(\"z\").output\n",
    "\n",
    "encoder = keras.Model(\n",
    "    inputs=[img_in_loaded, z_in_loaded],\n",
    "    outputs=z_latent_loaded,\n",
    "    name=\"encoder_from_ae\")\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatentSpace_pairplot(encoder_model, images, labels, n_dims=4):\n",
    "    # Encode all images to latent space\n",
    "    z = encoder_model.predict(images, batch_size=64)\n",
    "    \n",
    "    # If latent space > n_dims, only take the first few for clarity\n",
    "    z = z[:, :n_dims]\n",
    "    \n",
    "    # Build a dataframe\n",
    "    df = pd.DataFrame(z, columns=[f\"z{i+1}\" for i in range(n_dims)])\n",
    "    df[\"classification\"] = labels\n",
    "\n",
    "    # color and transparency maps\n",
    "    label_to_color = {\"LRD\": \"red\", \"Galaxy\": \"dodgerblue\", \"AGN\": \"mediumseagreen\"}\n",
    "    label_to_alpha = {\"LRD\": 1.0, \"Galaxy\": 0.15, \"AGN\": 0.45}\n",
    "\n",
    "    # Plot\n",
    "    sns.pairplot(df, hue=\"classification\", palette=label_to_color, plot_kws={'alpha':0.4, 's':20})\n",
    "    plt.suptitle(\"Latent Space Pairplot by Classification\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract classification labels\n",
    "labels = df_aug[\"classification\"].to_numpy()\n",
    "\n",
    "# Create visualization\n",
    "LatentSpace_pairplot(encoder, [aug_images, aug_redshifts], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "We use Uniform Manifold Approximation and Projection (UMAP) to reduce the dimensionality of the latent space to a 3D projection while preserving both the global structure (overall shape of the data) and local structure (how nearby points relate to each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have umap\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a known UMAP shadow module in `cuml` which introduces instability and crashes kernels when run on GPU. Run the following block as applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect which UMAP package you're actually using\n",
    "import umap\n",
    "print(\"UMAP path:\", umap.__file__)\n",
    "\n",
    "# If the printed path contains \"cuml\" or \"rapids\", then:\n",
    "# !pip uninstall cuml -y\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Get latent space from encoder ----------\n",
    "latent_space = encoder.predict([aug_images, aug_redshifts],\n",
    "                                batch_size=64)\n",
    "\n",
    "# ---------- 2) Run 2D UMAP ----------\n",
    "reducer_2d = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,      # <-- 2D now\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "latent_umap_2d = reducer_2d.fit_transform(latent_space)\n",
    "\n",
    "# ---------- 3) Put into DataFrame ----------\n",
    "df_latent_2d = pd.DataFrame(latent_umap_2d, columns=['UMAP1', 'UMAP2'])\n",
    "df_latent_2d['classification'] = df_aug['classification'].values\n",
    "\n",
    "# ---------- 4) Plot 2D scatter ----------\n",
    "label_to_color = {\"LRD\": \"red\", \"Galaxy\": \"dodgerblue\", \"AGN\": \"mediumseagreen\"}\n",
    "label_to_alpha = {\"LRD\": 1.0, \"Galaxy\": 0.15, \"AGN\": 0.45}\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "for cls in df_latent_2d['classification'].unique():\n",
    "    subset = df_latent_2d[df_latent_2d['classification'] == cls]\n",
    "    plt.scatter(\n",
    "        subset['UMAP1'],\n",
    "        subset['UMAP2'],\n",
    "        color=label_to_color[cls],\n",
    "        alpha=label_to_alpha[cls],\n",
    "        label=cls,\n",
    "        s=20\n",
    "    )\n",
    "\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.title('2D UMAP of latent space')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,      # local neighborhood size\n",
    "    min_dist=0.1,        # how tightly UMAP packs points\n",
    "    n_components=3,      # output dimensions (2D or 3D)\n",
    "    metric='euclidean')   # distance metric\n",
    "\n",
    "latent_space = encoder.predict([aug_images, aug_redshifts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reduced dimension latent space\n",
    "latent_umap = reducer.fit_transform(latent_space)\n",
    "latent_dims = ['UMAP1', 'UMAP2', 'UMAP3']\n",
    "\n",
    "df_latent = pd.DataFrame(latent_umap, columns=latent_dims)\n",
    "df_latent['classification'] = df_aug['classification']\n",
    "\n",
    "# color + alpha mapping to highlight LRDs\n",
    "label_to_color = {\"LRD\": \"red\", \"Galaxy\": \"dodgerblue\", \"AGN\": \"mediumseagreen\"}\n",
    "label_to_alpha = {\"LRD\": 1.0, \"Galaxy\": 0.15, \"AGN\": 0.45}\n",
    "\n",
    "# pairwise plots \n",
    "plot_pairs = [('UMAP1', 'UMAP2'), ('UMAP2', 'UMAP3'), ('UMAP1', 'UMAP3')]\n",
    "\n",
    "for x_dim, y_dim in plot_pairs:\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    \n",
    "    for cls in df_latent['classification'].unique():\n",
    "        subset = df_latent[df_latent['classification'] == cls]\n",
    "        plt.scatter(subset[x_dim], subset[y_dim],\n",
    "                    color=label_to_color[cls],\n",
    "                    alpha=label_to_alpha[cls],\n",
    "                    label=cls,\n",
    "                    s=20)\n",
    "    \n",
    "    plt.xlabel(x_dim)\n",
    "    plt.ylabel(y_dim)\n",
    "    plt.legend()\n",
    "    plt.title(f\"{x_dim} vs {y_dim}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot to see latent dimensions at once\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# create figure and 3D axes\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot for each classification to maintain different alpha levels\n",
    "for cls in df_latent['classification'].unique():\n",
    "    subset = df_latent[df_latent['classification'] == cls]\n",
    "    ax.scatter(subset['UMAP1'], subset['UMAP2'], subset['UMAP3'],\n",
    "                   c=label_to_color[cls],\n",
    "                   alpha=label_to_alpha[cls],\n",
    "                   label=cls,\n",
    "                   s=20)\n",
    "    \n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "ax.set_zlabel('UMAP3')\n",
    "ax.legend(title = 'Classification')\n",
    "ax.set_title('3D UMAP of Latent Space')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare latent space distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Make data frames for separate classes\n",
    "latent_LRD = df_latent[df_latent['classification'] == 'LRD'][['UMAP1','UMAP2','UMAP3']].values\n",
    "latent_Galaxy = df_latent[df_latent['classification'] == 'Galaxy'][['UMAP1','UMAP2','UMAP3']].values\n",
    "latent_AGN = df_latent[df_latent['classification'] == 'AGN'][['UMAP1','UMAP2','UMAP3']].values\n",
    "\n",
    "# Euclidean distances from each LRD to every point in the other groups\n",
    "dist_LRD_Galaxy = cdist(latent_LRD, latent_Galaxy)\n",
    "dist_LRD_AGN = cdist(latent_LRD, latent_AGN)\n",
    "\n",
    "# For each LRD, keep the nearest distance to each group\n",
    "df_LRD = df_latent[df_latent['classification'] == 'LRD'].copy()\n",
    "df_LRD['nearest_galaxy'] = dist_LRD_Galaxy.min(axis=1)\n",
    "df_LRD['nearest_agn'] = dist_LRD_AGN.min(axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "summary_panel = df_LRD[['nearest_galaxy', 'nearest_agn']].describe(percentiles=[.25, .5, .75]).T\n",
    "summary_panel.rename(columns={\n",
    "    '25%': 'Q1',\n",
    "    '50%': 'Median',\n",
    "    '75%': 'Q3'}, inplace=True)\n",
    "summary_panel = summary_panel[['min','Q1','Median','Q3','max','mean','std']]\n",
    "\n",
    "print(summary_panel)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
